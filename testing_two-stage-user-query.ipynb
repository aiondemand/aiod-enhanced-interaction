{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2-stage user-query building approach\n",
    "\n",
    "Steps:\n",
    "- STAGE 1: Extract conditions in natural language (spans from the input) corresponding to relevant metadata fields\n",
    "    - Output (simplified one):\n",
    "    ```python\n",
    "    [\n",
    "        (\"contains English and French\", \"languages\"),\n",
    "        (\"from Huggigface platform\", \"platform\"),\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "\n",
    "- STAGE 2: Process each natural language condition separately (we can dynamically create new output schemas)\n",
    "    - Output schema (for one condition):\n",
    "    ```python   \n",
    "    class Condition(BaseModel):\n",
    "        values: list[DYNAMIC_TYPE]\n",
    "        comp: Literal[\">\", \"<\", ...]\n",
    "        log_op: Literal[\"AND\", \"OR\"]\n",
    "    ```\n",
    "\n",
    "Problems:\n",
    "- It's not easy to model relationships (logical opetaros) between multiple Conditions associated with different types\n",
    "    - I suppose for now we will not support such use cases: E.g., \"Retrieve models that are either in Slovak or that have at least 1 million datapoints.\"\n",
    "    - We implicitly apply AND operator in between the Conditions. OR operator can only be applied in between values pertaining to a specific metadata field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompt_values import HumanMessage\n",
    "\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.chains.query_constructor.base import (\n",
    "    StructuredQueryOutputParser,\n",
    "    get_query_constructor_prompt\n",
    ")\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "from langchain_core.structured_query import Comparator, Operator\n",
    "from langchain.retrievers.self_query.milvus import MilvusTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "\n",
    "model = ChatOllama(model=MODEL_NAME, num_predict=4096, num_ctx=8192,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_metadata_filter import DatasetMetadataTemplate, Llama_ManualFunctionCalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class NaturalLanguageCondition(BaseModel):\n",
    "    condition: str = Field(..., description=\"Natural language condition corresponding to a particular metadata field we use for filtering\")\n",
    "    field: str = Field(..., description=\"Name of the metadata field\")\n",
    "\n",
    "class NaturalLanguageConditions(BaseModel):\n",
    "    \"\"\"Extraction of natural language conditions found within a user query\"\"\"\n",
    "    conditions: list[NaturalLanguageCondition] = Field(..., description=\"Natural language conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_types = {\n",
    "    \"platform\": \"string\",\n",
    "    \"date_published\": \"string\",\n",
    "    \"year\": \"integer\",\n",
    "    \"month\": \"integer\",\n",
    "    \"domains\": \"string\",\n",
    "    \"task_types\": \"string\",\n",
    "    \"license\": \"string\",\n",
    "    \"size_in_mb\": \"float\",\n",
    "    \"num_datapoints\": \"integer\",\n",
    "    \"size_category\": \"string\",\n",
    "    \"modalities\": \"string\",\n",
    "    \"data_formats\": \"string\",\n",
    "    \"languages\": \"string\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    {\n",
    "        \"name\": name, \n",
    "        \"description\": field.description, \n",
    "        \"type\": attribute_types[name]\n",
    "    } for name, field in DatasetMetadataTemplate.model_fields.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "    Your task is to extract user-defined conditions from a query, focusing on metadata fields relevant to filtering that are specified in the schema below. \n",
    "    Identify each condition explicitly mentioned in the query and assign it to the appropriate metadata field.\n",
    "\n",
    "    Extract the conditions only from the last user query as the other ones are used as an examples\n",
    "\n",
    "    A simple schema below briefly describes all the metadata fields we use for filtering purposes:\n",
    "    {model_schema}\n",
    "\n",
    "    **Instructions:**\n",
    "    1. Extract conditions as they appear in the query in their natural language form. You may slightly modify their word structure if necessary to perserve the logic regarding particular condition.\n",
    "    2. For each condition, determine the metadata field it pertains to, based on its meaning.\n",
    "    3. If a condition does not clearly pertain to a known metadata field, exclude it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Retrieve HuggingFace datasets about stocks\",\n",
    "        \"output\": {\n",
    "            \"conditions\": [\n",
    "                {\n",
    "                    \"condition\": \"HuggingFace datasets\", \n",
    "                    \"field\": \"platform\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Show me the summarization news datasets containing both the French as well as English data. The dataset however can't include any German data nor any Slovak data.\",\n",
    "        \"output\": {\n",
    "            \"conditions\": [\n",
    "                {\n",
    "                    \"condition\": \"summarization datasets\", \n",
    "                    \"field\": \"task_types\"\n",
    "                },\n",
    "                {\n",
    "                    \"condition\": \"containing both the French as well as English data\", \n",
    "                    \"field\": \"languages\"\n",
    "                },\n",
    "                {\n",
    "                    \"condition\": \"can\\'t include any German data nor any Slovak data\", \n",
    "                    \"field\": \"languages\"\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find all chocolate datasets created after January 1, 2022, that are represented in textual or image format with its dataset size smaller than 500 000KB.\",\n",
    "        \"output\": {\n",
    "            \"conditions\": [\n",
    "                {\n",
    "                    \"condition\": \"datasets created after January 1, 2022\", \n",
    "                    \"field\": \"date_published\"\n",
    "                },\n",
    "                {\n",
    "                    \"condition\": \"represented in textual or image format\", \n",
    "                    \"field\": \"modalities\"\n",
    "                },\n",
    "                {\n",
    "                    \"condition\": \"dataset size smaller than 500 000KB\", \n",
    "                    \"field\": \"size_in_mb\"\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Datasets that have either have over 50k datapoints but fewer than 100k, or datasets that have MIT or apache-2.0 license\",\n",
    "        \"output\": {\n",
    "            \"conditions\": [\n",
    "                {\n",
    "                    \"condition\": \"have over 50k datapoints but fewer than 100k\", \n",
    "                    \"field\": \"num_datapoints\"\n",
    "                },\n",
    "                {\n",
    "                    \"condition\": \"represented in textual or image format\", \n",
    "                    \"field\": \"modalities\"\n",
    "                },\n",
    "                {\n",
    "                    \"condition\": \"dataset size smaller than 500 000KB\", \n",
    "                    \"field\": \"size_in_mb\"\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Search for COVID-19 datasets\",\n",
    "        \"output\": {\n",
    "            \"conditions\": []\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"User Query: {query}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fewshot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_user_prompt = user_prompt.format(model_schema=json.dumps(metadata_field_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"system_prompt\"),\n",
    "        HumanMessage(modified_user_prompt),\n",
    "        # (\"user\", modified_user_prompt),\n",
    "        fewshot_prompt,\n",
    "        (\"human\", \"User Query: {query}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='system_prompt', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n    Your task is to extract user-defined conditions from a query, focusing on metadata fields relevant to filtering that are specified in the schema below. \\n    Identify each condition explicitly mentioned in the query and assign it to the appropriate metadata field.\\n\\n    Extract the conditions only from the last user query as the other ones are used as an examples\\n\\n    A simple schema below briefly describes all the metadata fields we use for filtering purposes:\\n    [{\"name\": \"platform\", \"description\": \"The platform where the asset is hosted. ONLY PERMITTED VALUES: [\\'huggingface\\', \\'openml\\', \\'zenodo\\']\", \"type\": \"string\"}, {\"name\": \"date_published\", \"description\": \"The original publication date of the asset in the format \\'YYYY-MM-DD\\'.\", \"type\": \"string\"}, {\"name\": \"year\", \"description\": \"The year extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"month\", \"description\": \"The month extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"domains\", \"description\": \"The AI technical domains of the asset, describing the type of data and AI task involved. ONLY PERMITTED VALUES: [\\'NLP\\', \\'Computer Vision\\', \\'Audio Processing\\']. Leave the list empty if not specified.\", \"type\": \"string\"}, {\"name\": \"task_types\", \"description\": \"The machine learning tasks supported by this asset. Acceptable values include task types found on HuggingFace (e.g., \\'token-classification\\', \\'question-answering\\', ...). Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"license\", \"description\": \"The license type governing the asset usage, if specified.\", \"type\": \"string\"}, {\"name\": \"size_in_mb\", \"description\": \"The total size of the dataset in megabytes. If the size is not explicitly specified in the dataset descritpion, sum up the sizes of individual files instead if possible. Don\\'t forget to convert the sizes to MBs\", \"type\": \"float\"}, {\"name\": \"num_datapoints\", \"description\": \"The number of data points in the dataset, if specified.\", \"type\": \"integer\"}, {\"name\": \"size_category\", \"description\": \"The general size category of the dataset, typically specified in ranges such as \\'10k<n<100k\\' found on HuggingFace. If you know the precise number of datapoints you may infer the size category.\", \"type\": \"string\"}, {\"name\": \"modalities\", \"description\": \"The modalities present in the dataset, such as \\'text\\', \\'tabular\\', \\'audio\\', \\'video\\', or \\'image\\'. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"data_formats\", \"description\": \"The file formats of the dataset (e.g., \\'CSV\\', \\'JSON\\', \\'Parquet\\'), if specified. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"languages\", \"description\": \"Languages present in the dataset, specified in ISO 639-1 two-letter codes (e.g., \\'EN\\' for English, \\'ES\\' for Spanish, \\'FR\\' for French, ...). Leave the list empty if not specified\", \"type\": \"string\"}]\\n\\n    **Instructions:**\\n    1. Extract conditions as they appear in the query in their natural language form. You may slightly modify their word structure if necessary to perserve the logic regarding particular condition.\\n    2. For each condition, determine the metadata field it pertains to, based on its meaning.\\n    3. If a condition does not clearly pertain to a known metadata field, exclude it.\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='User Query: Retrieve HuggingFace datasets about stocks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Extracted Conditions: {'conditions': [{'condition': 'HuggingFace datasets', 'field': 'platform'}]}\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"User Query: Show me the summarization news datasets containing both the French as well as English data. The dataset however can't include any German data nor any Slovak data.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Extracted Conditions: {\\'conditions\\': [{\\'condition\\': \\'summarization datasets\\', \\'field\\': \\'task_types\\'}, {\\'condition\\': \\'containing both the French as well as English data\\', \\'field\\': \\'languages\\'}, {\\'condition\\': \"can\\'t include any German data nor any Slovak data\", \\'field\\': \\'languages\\'}]}', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='User Query: Find all chocolate datasets created after January 1, 2022, that are represented in textual or image format with its dataset size smaller than 500 000KB.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Extracted Conditions: {'conditions': [{'condition': 'datasets created after January 1, 2022', 'field': 'date_published'}, {'condition': 'represented in textual or image format', 'field': 'modalities'}, {'condition': 'dataset size smaller than 500 000KB', 'field': 'size_in_mb'}]}\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='User Query: Datasets that have either have over 50k datapoints but fewer than 100k, or datasets that have MIT or apache-2.0 license', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Extracted Conditions: {'conditions': [{'condition': 'have over 50k datapoints but fewer than 100k', 'field': 'num_datapoints'}, {'condition': 'represented in textual or image format', 'field': 'modalities'}, {'condition': 'dataset size smaller than 500 000KB', 'field': 'size_in_mb'}]}\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='User Query: Search for COVID-19 datasets', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Extracted Conditions: {'conditions': []}\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='User Query: test', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt.invoke({\"query\": \"test\"}).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='system_prompt'), additional_kwargs={}),\n",
       " HumanMessage(content='\\n    Your task is to extract user-defined conditions from a query, focusing on metadata fields relevant to filtering that are specified in the schema below. \\n    Identify each condition explicitly mentioned in the query and assign it to the appropriate metadata field.\\n\\n    Extract the conditions only from the last user query as the other ones are used as an examples\\n\\n    A simple schema below briefly describes all the metadata fields we use for filtering purposes:\\n    [{\"name\": \"platform\", \"description\": \"The platform where the asset is hosted. ONLY PERMITTED VALUES: [\\'huggingface\\', \\'openml\\', \\'zenodo\\']\", \"type\": \"string\"}, {\"name\": \"date_published\", \"description\": \"The original publication date of the asset in the format \\'YYYY-MM-DD\\'.\", \"type\": \"string\"}, {\"name\": \"year\", \"description\": \"The year extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"month\", \"description\": \"The month extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"domains\", \"description\": \"The AI technical domains of the asset, describing the type of data and AI task involved. ONLY PERMITTED VALUES: [\\'NLP\\', \\'Computer Vision\\', \\'Audio Processing\\']. Leave the list empty if not specified.\", \"type\": \"string\"}, {\"name\": \"task_types\", \"description\": \"The machine learning tasks supported by this asset. Acceptable values include task types found on HuggingFace (e.g., \\'token-classification\\', \\'question-answering\\', ...). Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"license\", \"description\": \"The license type governing the asset usage, if specified.\", \"type\": \"string\"}, {\"name\": \"size_in_mb\", \"description\": \"The total size of the dataset in megabytes. If the size is not explicitly specified in the dataset descritpion, sum up the sizes of individual files instead if possible. Don\\'t forget to convert the sizes to MBs\", \"type\": \"float\"}, {\"name\": \"num_datapoints\", \"description\": \"The number of data points in the dataset, if specified.\", \"type\": \"integer\"}, {\"name\": \"size_category\", \"description\": \"The general size category of the dataset, typically specified in ranges such as \\'10k<n<100k\\' found on HuggingFace. If you know the precise number of datapoints you may infer the size category.\", \"type\": \"string\"}, {\"name\": \"modalities\", \"description\": \"The modalities present in the dataset, such as \\'text\\', \\'tabular\\', \\'audio\\', \\'video\\', or \\'image\\'. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"data_formats\", \"description\": \"The file formats of the dataset (e.g., \\'CSV\\', \\'JSON\\', \\'Parquet\\'), if specified. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"languages\", \"description\": \"Languages present in the dataset, specified in ISO 639-1 two-letter codes (e.g., \\'EN\\' for English, \\'ES\\' for Spanish, \\'FR\\' for French, ...). Leave the list empty if not specified\", \"type\": \"string\"}]\\n\\n    **Instructions:**\\n    1. Extract conditions as they appear in the query in their natural language form. You may slightly modify their word structure if necessary to perserve the logic regarding particular condition.\\n    2. For each condition, determine the metadata field it pertains to, based on its meaning.\\n    3. If a condition does not clearly pertain to a known metadata field, exclude it.\\n', additional_kwargs={}, response_metadata={}),\n",
       " FewShotChatMessagePromptTemplate(examples=[{'query': 'Retrieve HuggingFace datasets about stocks', 'output': {'conditions': [{'condition': 'HuggingFace datasets', 'field': 'platform'}]}}, {'query': \"Show me the summarization news datasets containing both the French as well as English data. The dataset however can't include any German data nor any Slovak data.\", 'output': {'conditions': [{'condition': 'summarization datasets', 'field': 'task_types'}, {'condition': 'containing both the French as well as English data', 'field': 'languages'}, {'condition': \"can't include any German data nor any Slovak data\", 'field': 'languages'}]}}, {'query': 'Find all chocolate datasets created after January 1, 2022, that are represented in textual or image format with its dataset size smaller than 500 000KB.', 'output': {'conditions': [{'condition': 'datasets created after January 1, 2022', 'field': 'date_published'}, {'condition': 'represented in textual or image format', 'field': 'modalities'}, {'condition': 'dataset size smaller than 500 000KB', 'field': 'size_in_mb'}]}}, {'query': 'Datasets that have either have over 50k datapoints but fewer than 100k, or datasets that have MIT or apache-2.0 license', 'output': {'conditions': [{'condition': 'have over 50k datapoints but fewer than 100k', 'field': 'num_datapoints'}, {'condition': 'represented in textual or image format', 'field': 'modalities'}, {'condition': 'dataset size smaller than 500 000KB', 'field': 'size_in_mb'}]}}, {'query': 'Search for COVID-19 datasets', 'output': {'conditions': []}}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['output', 'query'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='User Query: {query}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='Extracted Conditions: {output}'), additional_kwargs={})])),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='User Query: {input}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"name\": \"platform\", \"description\": \"The platform where the asset is hosted. ONLY PERMITTED VALUES: [\\'huggingface\\', \\'openml\\', \\'zenodo\\']\", \"type\": \"string\"}, {\"name\": \"date_published\", \"description\": \"The original publication date of the asset in the format \\'YYYY-MM-DD\\'.\", \"type\": \"string\"}, {\"name\": \"year\", \"description\": \"The year extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"month\", \"description\": \"The month extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"domains\", \"description\": \"The AI technical domains of the asset, describing the type of data and AI task involved. ONLY PERMITTED VALUES: [\\'NLP\\', \\'Computer Vision\\', \\'Audio Processing\\']. Leave the list empty if not specified.\", \"type\": \"string\"}, {\"name\": \"task_types\", \"description\": \"The machine learning tasks supported by this asset. Acceptable values include task types found on HuggingFace (e.g., \\'token-classification\\', \\'question-answering\\', ...). Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"license\", \"description\": \"The license type governing the asset usage, if specified.\", \"type\": \"string\"}, {\"name\": \"size_in_mb\", \"description\": \"The total size of the dataset in megabytes. If the size is not explicitly specified in the dataset descritpion, sum up the sizes of individual files instead if possible. Don\\'t forget to convert the sizes to MBs\", \"type\": \"float\"}, {\"name\": \"num_datapoints\", \"description\": \"The number of data points in the dataset, if specified.\", \"type\": \"integer\"}, {\"name\": \"size_category\", \"description\": \"The general size category of the dataset, typically specified in ranges such as \\'10k<n<100k\\' found on HuggingFace. If you know the precise number of datapoints you may infer the size category.\", \"type\": \"string\"}, {\"name\": \"modalities\", \"description\": \"The modalities present in the dataset, such as \\'text\\', \\'tabular\\', \\'audio\\', \\'video\\', or \\'image\\'. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"data_formats\", \"description\": \"The file formats of the dataset (e.g., \\'CSV\\', \\'JSON\\', \\'Parquet\\'), if specified. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"languages\", \"description\": \"Languages present in the dataset, specified in ISO 639-1 two-letter codes (e.g., \\'EN\\' for English, \\'ES\\' for Spanish, \\'FR\\' for French, ...). Leave the list empty if not specified\", \"type\": \"string\"}]'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(metadata_field_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = Llama_ManualFunctionCalling.populate_tool_prompt(NaturalLanguageConditions)\n",
    "user_prompt = user_prompt.format(\n",
    "    model_schema=json.dumps(metadata_field_info),\n",
    "    examples=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(system_prompt),\n",
    "    HumanMessage(user_prompt)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your task is to extract user-defined conditions from a query, focusing on metadata fields relevant to filtering that are specified in the schema below. \n",
      "Identify each condition explicitly mentioned in the query and assign it to the appropriate metadata field.\n",
      "\n",
      "Extract the conditions only from the last user query as the other ones are used as an examples\n",
      "\n",
      "A simple schema below briefly describes all the metadata fields we use for filtering purposes:\n",
      "[{\"name\": \"platform\", \"description\": \"The platform where the asset is hosted. ONLY PERMITTED VALUES: ['huggingface', 'openml', 'zenodo']\", \"type\": \"string\"}, {\"name\": \"date_published\", \"description\": \"The original publication date of the asset in the format 'YYYY-MM-DD'.\", \"type\": \"string\"}, {\"name\": \"year\", \"description\": \"The year extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"month\", \"description\": \"The month extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"domains\", \"description\": \"The AI technical domains of the asset, describing the type of data and AI task involved. ONLY PERMITTED VALUES: ['NLP', 'Computer Vision', 'Audio Processing']. Leave the list empty if not specified.\", \"type\": \"string\"}, {\"name\": \"task_types\", \"description\": \"The machine learning tasks supported by this asset. Acceptable values include task types found on HuggingFace (e.g., 'token-classification', 'question-answering', ...). Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"license\", \"description\": \"The license type governing the asset usage, if specified.\", \"type\": \"string\"}, {\"name\": \"size_in_mb\", \"description\": \"The total size of the dataset in megabytes. If the size is not explicitly specified in the dataset descritpion, sum up the sizes of individual files instead if possible. Don't forget to convert the sizes to MBs\", \"type\": \"float\"}, {\"name\": \"num_datapoints\", \"description\": \"The number of data points in the dataset, if specified.\", \"type\": \"integer\"}, {\"name\": \"size_category\", \"description\": \"The general size category of the dataset, typically specified in ranges such as '10k<n<100k' found on HuggingFace. If you know the precise number of datapoints you may infer the size category.\", \"type\": \"string\"}, {\"name\": \"modalities\", \"description\": \"The modalities present in the dataset, such as 'text', 'tabular', 'audio', 'video', or 'image'. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"data_formats\", \"description\": \"The file formats of the dataset (e.g., 'CSV', 'JSON', 'Parquet'), if specified. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"languages\", \"description\": \"Languages present in the dataset, specified in ISO 639-1 two-letter codes (e.g., 'EN' for English, 'ES' for Spanish, 'FR' for French, ...). Leave the list empty if not specified\", \"type\": \"string\"}]\n",
      "\n",
      "**Instructions:**\n",
      "1. Extract conditions as they appear in the query in their natural language form. You may slightly modify their word structure if necessary to perserve the logic regarding particular condition.\n",
      "2. For each condition, determine the metadata field it pertains to, based on its meaning.\n",
      "3. If a condition does not clearly pertain to a known metadata field, exclude it.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "\n",
      "**Query to extract conditions from:**\n",
      "User Query:\n",
      "{query}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYour task is to extract user-defined conditions from a query, focusing on metadata fields relevant to filtering that are specified in the schema below. \\nIdentify each condition explicitly mentioned in the query and assign it to the appropriate metadata field.\\n\\nExtract the conditions only from the last user query as the other ones are used as an examples\\n\\nA simple schema below briefly describes all the metadata fields we use for filtering purposes:\\n[{\"name\": \"platform\", \"description\": \"The platform where the asset is hosted. ONLY PERMITTED VALUES: [\\'huggingface\\', \\'openml\\', \\'zenodo\\']\", \"type\": \"string\"}, {\"name\": \"date_published\", \"description\": \"The original publication date of the asset in the format \\'YYYY-MM-DD\\'.\", \"type\": \"string\"}, {\"name\": \"year\", \"description\": \"The year extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"month\", \"description\": \"The month extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"domains\", \"description\": \"The AI technical domains of the asset, describing the type of data and AI task involved. ONLY PERMITTED VALUES: [\\'NLP\\', \\'Computer Vision\\', \\'Audio Processing\\']. Leave the list empty if not specified.\", \"type\": \"string\"}, {\"name\": \"task_types\", \"description\": \"The machine learning tasks supported by this asset. Acceptable values include task types found on HuggingFace (e.g., \\'token-classification\\', \\'question-answering\\', ...). Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"license\", \"description\": \"The license type governing the asset usage, if specified.\", \"type\": \"string\"}, {\"name\": \"size_in_mb\", \"description\": \"The total size of the dataset in megabytes. If the size is not explicitly specified in the dataset descritpion, sum up the sizes of individual files instead if possible. Don\\'t forget to convert the sizes to MBs\", \"type\": \"float\"}, {\"name\": \"num_datapoints\", \"description\": \"The number of data points in the dataset, if specified.\", \"type\": \"integer\"}, {\"name\": \"size_category\", \"description\": \"The general size category of the dataset, typically specified in ranges such as \\'10k<n<100k\\' found on HuggingFace. If you know the precise number of datapoints you may infer the size category.\", \"type\": \"string\"}, {\"name\": \"modalities\", \"description\": \"The modalities present in the dataset, such as \\'text\\', \\'tabular\\', \\'audio\\', \\'video\\', or \\'image\\'. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"data_formats\", \"description\": \"The file formats of the dataset (e.g., \\'CSV\\', \\'JSON\\', \\'Parquet\\'), if specified. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"languages\", \"description\": \"Languages present in the dataset, specified in ISO 639-1 two-letter codes (e.g., \\'EN\\' for English, \\'ES\\' for Spanish, \\'FR\\' for French, ...). Leave the list empty if not specified\", \"type\": \"string\"}]\\n\\n**Instructions:**\\n1. Extract conditions as they appear in the query in their natural language form. You may slightly modify their word structure if necessary to perserve the logic regarding particular condition.\\n2. For each condition, determine the metadata field it pertains to, based on its meaning.\\n3. If a condition does not clearly pertain to a known metadata field, exclude it.\\n\\n**Examples:**\\n\\n\\n**Query to extract conditions from:**\\nUser Query:\\n{query}\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessage(content='\\n        You have access to the following functions:\\n\\n        Use the function \\'NaturalLanguageConditions\\' to \\'Extraction of natural language conditions found within a user query\\':\\n        {\"$defs\": {\"NaturalLanguageCondition\": {\"properties\": {\"condition\": {\"description\": \"Natural language condition corresponding to a particular metadata field we use for filtering\", \"title\": \"Condition\", \"type\": \"string\"}, \"field\": {\"description\": \"Name of the metadata field\", \"title\": \"Field\", \"type\": \"string\"}}, \"required\": [\"condition\", \"field\"], \"title\": \"NaturalLanguageCondition\", \"type\": \"object\"}}, \"description\": \"Extraction of natural language conditions found within a user query\", \"name\": \"NaturalLanguageConditions\", \"parameters\": {\"type\": \"object\", \"properties\": {\"conditions\": {\"description\": \"Natural language conditions\", \"items\": {\"$ref\": \"#/$defs/NaturalLanguageCondition\"}, \"title\": \"Conditions\", \"type\": \"array\"}}, \"required\": [\"conditions\"]}}\\n\\n        If you choose to call a function ONLY reply in the following format with no prefix or suffix:\\n\\n        <function=example_function_name>{\"example_name\": \"example_value\"}</function>\\n\\n        Reminder:\\n        - Function calls MUST follow the specified format, start with <function= and end with </function>\\n        - Required parameters MUST be specified\\n        - Only call one function at a time\\n        - Put the entire function call reply on one line\\n        - If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\\n    ', additional_kwargs={}, response_metadata={}), HumanMessage(content='\\nYour task is to extract user-defined conditions from a query, focusing on metadata fields relevant to filtering that are specified in the schema below. \\nIdentify each condition explicitly mentioned in the query and assign it to the appropriate metadata field.\\n\\nExtract the conditions only from the last user query as the other ones are used as an examples\\n\\nA simple schema below briefly describes all the metadata fields we use for filtering purposes:\\n[{\"name\": \"platform\", \"description\": \"The platform where the asset is hosted. ONLY PERMITTED VALUES: [\\'huggingface\\', \\'openml\\', \\'zenodo\\']\", \"type\": \"string\"}, {\"name\": \"date_published\", \"description\": \"The original publication date of the asset in the format \\'YYYY-MM-DD\\'.\", \"type\": \"string\"}, {\"name\": \"year\", \"description\": \"The year extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"month\", \"description\": \"The month extracted from the publication date.\", \"type\": \"integer\"}, {\"name\": \"domains\", \"description\": \"The AI technical domains of the asset, describing the type of data and AI task involved. ONLY PERMITTED VALUES: [\\'NLP\\', \\'Computer Vision\\', \\'Audio Processing\\']. Leave the list empty if not specified.\", \"type\": \"string\"}, {\"name\": \"task_types\", \"description\": \"The machine learning tasks supported by this asset. Acceptable values include task types found on HuggingFace (e.g., \\'token-classification\\', \\'question-answering\\', ...). Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"license\", \"description\": \"The license type governing the asset usage, if specified.\", \"type\": \"string\"}, {\"name\": \"size_in_mb\", \"description\": \"The total size of the dataset in megabytes. If the size is not explicitly specified in the dataset descritpion, sum up the sizes of individual files instead if possible. Don\\'t forget to convert the sizes to MBs\", \"type\": \"float\"}, {\"name\": \"num_datapoints\", \"description\": \"The number of data points in the dataset, if specified.\", \"type\": \"integer\"}, {\"name\": \"size_category\", \"description\": \"The general size category of the dataset, typically specified in ranges such as \\'10k<n<100k\\' found on HuggingFace. If you know the precise number of datapoints you may infer the size category.\", \"type\": \"string\"}, {\"name\": \"modalities\", \"description\": \"The modalities present in the dataset, such as \\'text\\', \\'tabular\\', \\'audio\\', \\'video\\', or \\'image\\'. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"data_formats\", \"description\": \"The file formats of the dataset (e.g., \\'CSV\\', \\'JSON\\', \\'Parquet\\'), if specified. Leave the list empty if not specified\", \"type\": \"string\"}, {\"name\": \"languages\", \"description\": \"Languages present in the dataset, specified in ISO 639-1 two-letter codes (e.g., \\'EN\\' for English, \\'ES\\' for Spanish, \\'FR\\' for French, ...). Leave the list empty if not specified\", \"type\": \"string\"}]\\n\\n**Instructions:**\\n1. Extract conditions as they appear in the query in their natural language form. You may slightly modify their word structure if necessary to perserve the logic regarding particular condition.\\n2. For each condition, determine the metadata field it pertains to, based on its meaning.\\n3. If a condition does not clearly pertain to a known metadata field, exclude it.\\n\\n**Examples:**\\n\\n\\n**Query to extract conditions from:**\\nUser Query:\\n{query}\\n', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO incorporate few shot examples into the user prompt + inject information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = (\n",
    "    \"Retrieve all the translation Stanford datasets with at least 10k datapoints and has over 100k KB in size\" +\n",
    "    \"and the dataset should have contain Slovak language, Polish language, but no Czech language.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tailor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
