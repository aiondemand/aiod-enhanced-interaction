services:
  # FastAPI service
  app:
    build:
      context: .
      dockerfile: Dockerfile.final
      args:
        USER_UID: ${USER_UID}
        USER_GID: ${USER_GID}
    labels:
      autoheal-label: true
    {% if USE_MILVUS_LITE == "true" %}
    command: /bin/sh -c "uvicorn app.main:app --host 0.0.0.0 --port 80"
    {% else %}
    command: /bin/sh -c "python scripts/milvus_credentials_setup.py && uvicorn app.main:app --host 0.0.0.0 --port 80"
    {% endif %}
    env_file:
      - .env.app
    environment:
      - USE_GPU={{ USE_GPU }}
      - MILVUS__USE_LITE={{ USE_MILVUS_LITE }}
      {% if USE_MILVUS_LITE == "true" %}
      - MILVUS__FILEPATH=/data/milvus.db
      - METADATA_FILTERING__ENABLED=false
      {% else %}
      - MILVUS__URI=http://milvus-standalone:19530
      - MILVUS__USER=${MILVUS_AIOD_USER}
      - MILVUS__PASS=${MILVUS_AIOD_PASS}
      - MILVUS_NEW_ROOT_PASS=${MILVUS_NEW_ROOT_PASS}
      - METADATA_FILTERING__ENABLED={{ USE_LLM }}
      {% endif %}
      - MONGO__HOST=mongo
      - MONGO__PORT=27017
      - MONGO__USER=${MONGO_USER}
      - MONGO__PASSWORD=${MONGO_PASSWORD}
      - MONGO__DBNAME=aiod
      - CHATBOT__USE_CHATBOT={{ USE_CHATBOT }}
      - AIOD__JSON_SAVEPATH=/cold_data
      {% if USE_LLM == "true" %}
      - OLLAMA__URI=http://ollama:11434
      {% endif %}
    ports:
      - "${APP_HOST_PORT:-8000}:80"
    depends_on:
      {% if USE_MILVUS_LITE == "false" %}
      milvus-standalone:
        condition: service_healthy
        restart: false
      {% endif %}
      mongo:
        condition: service_healthy
        restart: false
      {% if USE_LLM == "true" %}
      ollama:
        condition: service_healthy
        restart: false
      {% endif %}
    volumes:
      - ${DATA_DIRPATH}/model:/model
      - ${DATA_DIRPATH}/cold_data:/cold_data
      {% if USE_MILVUS_LITE == "true" %}
      - ${DATA_DIRPATH}/volumes:/data
      {% endif %}
    healthcheck:
      # TODO GPU specific healthcheck shouldn't be necessary once we resolve issue:
      # https://github.com/aiondemand/aiod-enhanced-interaction/issues/76
      {% if USE_GPU == "true" %}
      test: ["CMD-SHELL", "curl -f http://localhost:80/health && nvidia-smi || exit 1"]
      {% else %}
      test: ["CMD-SHELL", "curl -f http://localhost:80/health"]
      {% endif %}
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 120s
    restart: always
    {% if USE_GPU == "true" %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    {% endif %}
  {% if USE_LLM == "true" %}

  #Ollama service
  ollama:
    image: ollama/ollama:0.11.0
    labels:
      autoheal-label: true
    restart: always
    healthcheck:
      # TODO Same as above
      {% if USE_GPU == "true" %}
      test: ["CMD-SHELL", "ollama && nvidia-smi || exit 1"]
      {% else %}
      test: ["CMD", "ollama"]
      {% endif %}
      interval: 30s
      timeout: 20s
      retries: 3
    volumes:
      - ${DATA_DIRPATH}/ollama:/root/.ollama/
    ports:
      - "${OLLAMA_HOST_PORT:-11434}:11434"
    {% if USE_GPU == "true" %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    {% endif %}
  {% endif %}
