services:
  # FastAPI service
  app:
    build:
      context: .
      dockerfile: Dockerfile.final
      args:
        USER_UID: ${USER_UID}
        USER_GID: ${USER_GID}
    labels:
      autoheal-label: true
    command: /bin/sh -c "python scripts/milvus_credentials_setup.py && uvicorn app.main:app --host 0.0.0.0 --port 80"
    env_file:
      - .env.app
    environment:
      - TINYDB_FILEPATH=/data/tinydb.json
      - USE_GPU={{ USE_GPU }}
      - MILVUS__URI=http://milvus-standalone:19530
      - MILVUS__USER=${MILVUS_AIOD_USER}
      - MILVUS__PASS=${MILVUS_AIOD_PASS}
      - MILVUS__EXTRACT_METADATA={{ USE_LLM }}
      - MILVUS_NEW_ROOT_PASS=${MILVUS_NEW_ROOT_PASS}
      {% if USE_LLM == "true" %}
      - OLLAMA__URI=http://ollama:11434
      {% endif %}
    ports:
      - "${APP_HOST_PORT:-8000}:80"
    depends_on:
      milvus-standalone:
        condition: service_healthy
        restart: false
      {% if USE_LLM == "true" %}
      ollama:
        condition: service_healthy
        restart: false
      {% endif %}
    volumes:
      - ${DATA_DIRPATH}/volumes/tinydb:/data
      - ${DATA_DIRPATH}/model:/model
    healthcheck:
      # TODO GPU specific healthcheck shouldn't be necessary once we resolve issue:
      # https://github.com/aiondemand/aiod-enhanced-interaction/issues/76
      {% if USE_GPU == "true" %}
      test: ["CMD-SHELL", "curl -f http://localhost:80/health && nvidia-smi || exit 1"]
      {% else %}
      test: ["CMD-SHELL", "curl -f http://localhost:80/health"]
      {% endif %}
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 30s
    restart: always
    {% if USE_GPU == "true" %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    {% endif %}
  {% if USE_LLM == "true" %}

  #Ollama service
  ollama:
    image: ollama/ollama:0.5.4
    labels:
      autoheal-label: true
    restart: always
    healthcheck:
      # TODO Same as above
      {% if USE_GPU == "true" %}
      test: ["CMD-SHELL", "ollama && nvidia-smi || exit 1"]
      {% else %}
      test: ["CMD", "ollama"]
      {% endif %}
      interval: 30s
      timeout: 20s
      retries: 3
    volumes:
      - ${DATA_DIRPATH}/ollama:/root/.ollama/
    ports:
      - "${OLLAMA_HOST_PORT:-11434}:11434"
    {% if USE_GPU == "true" %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    {% endif %}
  {% endif %}
