apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "aiod-enhanced-interaction.ollama.fullname" . }}
  labels:
    {{- include "aiod-enhanced-interaction.ollama.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.ollama.replicas }}
  selector:
    matchLabels:
      {{- include "aiod-enhanced-interaction.ollama.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "aiod-enhanced-interaction.ollama.selectorLabels" . | nindent 8 }}
    spec:
      containers:
      - name: ollama
        image: "{{ .Values.ollama.image.repository }}:{{ .Values.ollama.image.tag }}"
        ports:
        - containerPort: 11434
          name: ollama
        volumeMounts:
        - name: ollama-data
          mountPath: /root/.ollama/
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
          limits:
            memory: "8Gi"
            cpu: "2000m"
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - "ollama list || exit 1"
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - "ollama list || exit 1"
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
      volumes:
      - name: ollama-data
        persistentVolumeClaim:
          claimName: {{ include "aiod-enhanced-interaction.ollama.fullname" . }}-pvc
      # Uncomment the following section if GPU support is needed
      # nodeSelector:
      #   accelerator: nvidia-tesla-k80  # Adjust based on your GPU node labels
      # tolerations:
      # - key: nvidia.com/gpu
      #   operator: Exists
      #   effect: NoSchedule
      # If GPU is available, add:
      # resources:
      #   limits:
      #     nvidia.com/gpu: 1
